{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "hw_1_update_fin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1yLHjQ3Jw89",
        "outputId": "950e4bd9-9c48-4ae7-b6fc-055757bb1bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random \n",
        "from fake_useragent import UserAgent\n",
        "ua = UserAgent(verify_ssl=False)\n",
        "session = requests.session()\n",
        "headers = {'User-Agent': ua.random}\n",
        "from string import punctuation\n",
        "punctuation = punctuation + '—–“”«»'\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()\n",
        "from sklearn.metrics import accuracy_score\n",
        "stopWords = stopwords.words('russian')\n",
        "from collections import Counter"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQhi46-Jw9S"
      },
      "source": [
        "def page(url, headers):\n",
        "    posts_list = []\n",
        "    response = session.get(url, headers=headers)\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    posts = soup.find_all('span', {'class': '_reachbanner_'})\n",
        "    for post in posts:\n",
        "        i = post.get_text()\n",
        "        posts_list.append(i)\n",
        "    return posts_list[:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00YZsvjcJw9e"
      },
      "source": [
        "def clean(posts):\n",
        "    clean_rew = []\n",
        "    for_train = []\n",
        "    for_test = []\n",
        "    for r in posts:\n",
        "        r = r.replace('\\n', '')\n",
        "        r = r.replace('\\r', '')\n",
        "        r = r.lower()\n",
        "        small_list = []\n",
        "        for p in punctuation:\n",
        "            if p in r:\n",
        "                r = r.replace(p, ' ')\n",
        "        w = nltk.word_tokenize(r)\n",
        "        for elem in w:\n",
        "            ana = morph.parse(elem) \n",
        "            n = ana[0].normal_form\n",
        "            if n not in stopWords or n == 'не':\n",
        "                small_list.append(n)\n",
        "        clean_rew.append(small_list)\n",
        "    for s in clean_rew[:30]:\n",
        "        for c in s:\n",
        "            for_train.append(c)\n",
        "    for s in clean_rew[30:40]:\n",
        "        for_test.append(s)\n",
        "    return for_train, for_test"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5s7LlmJJw9r"
      },
      "source": [
        "def unique(first_list, sec_list):\n",
        "    unique_list = []\n",
        "    clean_unique_list = []\n",
        "    for i in first_list:\n",
        "        if i not in sec_list:\n",
        "            unique_list.append(i)\n",
        "    for c in Counter(unique_list).items():\n",
        "        if c[1]>2:\n",
        "            clean_unique_list.append(c[0])\n",
        "    return clean_unique_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiuLzKdbJw95"
      },
      "source": [
        "def simple_type_detect(words_list, bad_words, good_words):\n",
        "    word_scores = {'negative':0, 'positive':0}\n",
        "    for i in words_list:\n",
        "        if i in bad_words:\n",
        "            word_scores['negative'] += 1\n",
        "        elif i in good_words:\n",
        "            word_scores['positive'] += 1\n",
        "    return Counter(word_scores).most_common(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knpeZxkwJw-F"
      },
      "source": [
        "def accuracy(tests, first_uniques, second_uniques):\n",
        "    gold = []\n",
        "    result = []\n",
        "    for t in tests.items():\n",
        "        gold.append(t[1])\n",
        "        n = simple_type_detect(t[0], first_uniques, second_uniques) \n",
        "        result.append(n[0][0])\n",
        "    return accuracy_score(result, gold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNSa_OOrJw-b"
      },
      "source": [
        "url_bad = 'https://www.kinopoisk.ru/film/462682/reviews/ord/rating/status/bad/perpage/100/'\n",
        "url_good = 'https://www.kinopoisk.ru/film/462682/reviews/ord/rating/status/good/perpage/200/'"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGbeH9fkJw-q"
      },
      "source": [
        "bad = clean(page(url_bad, headers))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmcfPlHJw-3"
      },
      "source": [
        "good = clean(page(url_good, headers))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No3zSWMKJw_B"
      },
      "source": [
        "test_rews = {}\n",
        "for b in bad[1]:\n",
        "    test_rews[tuple(b)] = 'negative'\n",
        "for g in good[1]:\n",
        "    test_rews[tuple(g)] = 'positive'"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQnmDiLKJw_V"
      },
      "source": [
        "bad_uniques = unique(bad[0], good[0])"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdttIt4XJw_e"
      },
      "source": [
        "good_uniques = unique(good[0], bad[0])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SETnPgQJw_n",
        "outputId": "4894564a-d81b-41ea-9e36-06a37de36bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(test_rews, bad_uniques, good_uniques)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b_qeRnsJw_1"
      },
      "source": [
        "#среди плохих тестовых рецензий правильно определяется только 6/10\n",
        "#среди хороших тестовых рецензий правильно определяется 10/10\n",
        "#видимо так проиходит потому что больше уникальных слов у хороших рецензий"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3SX5NkCJxAA"
      },
      "source": [
        "# Как улучшить?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H6uFeVQJxAC"
      },
      "source": [
        "1. Можно воспользоваться нейросетью и обучить ее на большем количестве текстов (плохих и хороших; пусть еще смотрит не слова, а словосочетания; определяет синонимы), брать не только один фильм, и вообще брать разные типы текстов\n",
        "2. Можно получше убрать шум (слова с опечатками, имена героев если они остаются и тд)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtZeKAgaMdr_"
      },
      "source": [
        "## Улучшение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YghDXSJzQzQu"
      },
      "source": [
        "Лучше всех с русским текстом работал pymystem3, поэтому берем его"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ-hz4DMJxAF",
        "outputId": "9583f876-a001-4679-9b9c-c80c42c84924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem = Mystem()\n",
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 20:01:50--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.243, 5.45.205.245, 5.45.205.242, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-mskstoredata10.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
            "--2020-10-18 20:01:51--  http://cache-mskstoredata10.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving cache-mskstoredata10.cdn.yandex.net (cache-mskstoredata10.cdn.yandex.net)... 37.9.96.21, 2a02:6b8:0:3706::19\n",
            "Connecting to cache-mskstoredata10.cdn.yandex.net (cache-mskstoredata10.cdn.yandex.net)|37.9.96.21|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz.1’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  8.74MB/s    in 1.8s    \n",
            "\n",
            "2020-10-18 20:01:53 (8.74 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz.1’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZO01DGoA-XK"
      },
      "source": [
        "Синтаксические группы:\n",
        "*   Местоимение + Глагол (для таких штук: Я люблю / Я ненавижу, просто отследить эмоции рецензента) \n",
        "*   Наречие + Прилагательное (для таких штук: Очень крутой / Немножко скучный, фильм может быть очень хорошим но немножко скучным, такое наверное встретится только в хорошем отзыве, в плохом написали бы, что он очень скучный)\n",
        "*   Не + Глагол (для штук типа: Не люблю / Не напрягает, чтобы различать хорошее и плохое отношение человека к фильму)\n",
        "\n",
        "Такие группы помогут взглянут на картину более целостно и точно\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAaXYO1SOEST"
      },
      "source": [
        "def pos(word_list):\n",
        "  mystem_tags = []\n",
        "  for i in word_list:\n",
        "    m = mystem.analyze(i)\n",
        "    try:\n",
        "      gr = m[0]['analysis'][0]['gr']\n",
        "      pos = gr.split('=')[0].split(',')[0]\n",
        "    except:\n",
        "      pos = ''\n",
        "    mystem_tags.append(pos)\n",
        "  return mystem_tags"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkhEYBKYGkRY"
      },
      "source": [
        "def syntax_groups(words_list, pos_list):\n",
        "  group_prontoverb = []\n",
        "  group_advtoadj = []\n",
        "  group_netoverb = []\n",
        "  for t in range(len(pos_list)):\n",
        "    if pos_list[t]=='SPRO' and pos_list[t+1]=='V':\n",
        "      group_prontoverb.append(words_list[t] + ' ' + words_list[t+1])\n",
        "    if pos_list[t]=='ADV' and pos_list[t+1]=='A':\n",
        "      group_advtoadj.append(words_list[t] + ' ' + words_list[t+1])\n",
        "    if words_list[t]=='не' and pos_list[t+1]=='V':\n",
        "      group_netoverb.append(words_list[t] + ' ' + words_list[t+1])\n",
        "  return group_prontoverb, group_advtoadj, group_netoverb"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjUe5L9tawGU"
      },
      "source": [
        "def groups_unique(first_list, sec_list):\n",
        "    unique_list = []\n",
        "    for i in first_list:\n",
        "        if i not in sec_list:\n",
        "            unique_list.append(i)\n",
        "    return unique_list"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk1FCK2PZTLe"
      },
      "source": [
        "def simple_type_detect_new(words_str, bad_words, good_words):\n",
        "  word_scores = {'negative':0, 'positive':0}\n",
        "  for i in bad_words:\n",
        "    if i in words_str:\n",
        "      word_scores['negative'] += 1\n",
        "  for m in good_words:\n",
        "    if m in words_str:\n",
        "      print(m)\n",
        "      word_scores['positive'] += 1\n",
        "  return Counter(word_scores).most_common(1)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URvicBWmTSsR"
      },
      "source": [
        "train_good_pos = pos(good[0])\n",
        "train_bad_pos = pos(bad[0])"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd7g9jJafAfv"
      },
      "source": [
        "good_syntax_groups = syntax_groups(good[0], train_good_pos)\n",
        "bad_syntax_groups = syntax_groups(bad[0], train_bad_pos)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2LmCyRrifm2"
      },
      "source": [
        "unique_good_bigrams = set(groups_unique(good_syntax_groups[0], bad_syntax_groups[0]) + groups_unique(good_syntax_groups[1], bad_syntax_groups[1]) + \n",
        "                          groups_unique(good_syntax_groups[2], bad_syntax_groups[2]))"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGCGBjipTkS2"
      },
      "source": [
        "unique_bad_bigrams = set(groups_unique(bad_syntax_groups[0], good_syntax_groups[0]) + groups_unique(bad_syntax_groups[1], good_syntax_groups[1]) + \n",
        "                          groups_unique(bad_syntax_groups[2], good_syntax_groups[2]))"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu-OG-S3WB51"
      },
      "source": [
        "test_rews_new = {}\n",
        "for b in bad[1]:\n",
        "  n = ''\n",
        "  for m in b:\n",
        "    n += m + ' '\n",
        "  test_rews_new[n] = 'negative'\n",
        "for g in good[1]:\n",
        "  n2 = ''\n",
        "  for k in g:\n",
        "    n2 += k + ' '\n",
        "  test_rews_new[n2] = 'positive'   "
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAGpNyXrUZ6S",
        "outputId": "cea21bb4-b97b-452d-c840-7bfbd0274d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(test_rews_new, unique_bad_bigrams, unique_good_bigrams)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCZgIwCVbzVN"
      },
      "source": [
        "Качество упало:("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG485q0Eb3dw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}