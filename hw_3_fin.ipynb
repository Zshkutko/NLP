{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "hw_3_fin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy7K0UQwzAXL",
        "outputId": "a2a8ea67-f3fb-449b-c239-15429948e571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "!python3 -m spacy download en\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "import spacy\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_idf_vectorize = TfidfVectorizer(use_idf=True)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Y11wxizotL",
        "outputId": "a22b887b-7a45-41b4-e93f-74dc61b8a8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.target_names.unique())\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
            " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
            " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
            " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
            " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
            " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ...           target_names\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...              rec.autos\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...  comp.sys.mac.hardware\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...  comp.sys.mac.hardware\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...          comp.graphics\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...              sci.space\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXL54A8tnFvN",
        "outputId": "9e6fd681-0fdd-4b37-e632-4d673c66328e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "data = df.content.values.tolist()\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "pprint(data[:1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
            " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
            " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
            " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
            " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
            " 'addition, the front bumper was separate from the rest of the body. This is '\n",
            " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
            " 'production, where this car is made, history, or whatever info you have on '\n",
            " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
            " 'your neighborhood Lerxst ---- ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C62AU0DxnONX",
        "outputId": "e6fa13b7-6e86-474f-e9f7-e380afcf12c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WNAmbDTN7R0",
        "outputId": "0bfd52bf-5a84-4e27-c255-8a7fad04e55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoqXkd14OZmn"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZmZsFUTOmQt",
        "outputId": "9411db33-c7c9-4d25-c3e6-0ba02e8d398b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data_words_nostops = remove_stopwords(data_words)\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['where', 'thing', 'car', 'nntp_poste', 'host', 'park', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmEZgugXO6Vj",
        "outputId": "f4f3506b-31ea-46c8-83b4-0820336aac6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus[:1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQJD3Sd1PM3z",
        "outputId": "48f31034-a3e8-43a2-f3b2-49e8ea5b968d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('addition', 1),\n",
              "  ('body', 1),\n",
              "  ('bricklin', 1),\n",
              "  ('bring', 1),\n",
              "  ('call', 1),\n",
              "  ('car', 5),\n",
              "  ('could', 1),\n",
              "  ('day', 1),\n",
              "  ('door', 2),\n",
              "  ('early', 1),\n",
              "  ('engine', 1),\n",
              "  ('enlighten', 1),\n",
              "  ('funky', 1),\n",
              "  ('history', 1),\n",
              "  ('host', 1),\n",
              "  ('info', 1),\n",
              "  ('know', 1),\n",
              "  ('late', 1),\n",
              "  ('lerxst', 1),\n",
              "  ('line', 1),\n",
              "  ('look', 2),\n",
              "  ('mail', 1),\n",
              "  ('make', 1),\n",
              "  ('model', 1),\n",
              "  ('name', 1),\n",
              "  ('neighborhood', 1),\n",
              "  ('nntp_poste', 1),\n",
              "  ('park', 1),\n",
              "  ('production', 1),\n",
              "  ('really', 1),\n",
              "  ('rest', 1),\n",
              "  ('see', 1),\n",
              "  ('separate', 1),\n",
              "  ('small', 1),\n",
              "  ('sport', 1),\n",
              "  ('tellme', 1),\n",
              "  ('thank', 1),\n",
              "  ('thing', 1),\n",
              "  ('where', 1),\n",
              "  ('wonder', 1),\n",
              "  ('year', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcQHTQCJPRLE"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcmnC5FLPTsv",
        "outputId": "13a0b6a0-a289-4c59-a273-e53abbf2c85e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.051*\"report\" + 0.027*\"black\" + 0.020*\"fire\" + 0.020*\"white\" + '\n",
            "  '0.016*\"trial\" + 0.016*\"cover\" + 0.015*\"medium\" + 0.013*\"vote\" + '\n",
            "  '0.012*\"minor\" + 0.012*\"title\"'),\n",
            " (1,\n",
            "  '0.021*\"god\" + 0.020*\"accept\" + 0.016*\"member\" + 0.015*\"man\" + '\n",
            "  '0.014*\"israeli\" + 0.014*\"season\" + 0.012*\"publish\" + 0.012*\"lebanese\" + '\n",
            "  '0.012*\"jewish\" + 0.011*\"brain\"'),\n",
            " (2,\n",
            "  '0.017*\"package\" + 0.016*\"press\" + 0.015*\"item\" + 0.015*\"break\" + '\n",
            "  '0.011*\"level\" + 0.010*\"edge\" + 0.009*\"hole\" + 0.007*\"eye\" + '\n",
            "  '0.007*\"equipment\" + 0.007*\"contribute\"'),\n",
            " (3,\n",
            "  '0.025*\"pc\" + 0.022*\"contain\" + 0.020*\"input\" + 0.020*\"reality\" + '\n",
            "  '0.017*\"picture\" + 0.016*\"object\" + 0.016*\"level\" + 0.015*\"box\" + '\n",
            "  '0.015*\"quality\" + 0.013*\"greek\"'),\n",
            " (4,\n",
            "  '0.089*\"ax\" + 0.076*\"max\" + 0.032*\"space\" + 0.021*\"launch\" + 0.018*\"di_di\" + '\n",
            "  '0.017*\"orbit\" + 0.016*\"sphere\" + 0.015*\"satellite\" + 0.014*\"plane\" + '\n",
            "  '0.014*\"mission\"'),\n",
            " (5,\n",
            "  '0.019*\"people\" + 0.017*\"kill\" + 0.015*\"child\" + 0.015*\"government\" + '\n",
            "  '0.012*\"attack\" + 0.012*\"year\" + 0.012*\"die\" + 0.011*\"country\" + 0.010*\"say\" '\n",
            "  '+ 0.009*\"war\"'),\n",
            " (6,\n",
            "  '0.035*\"window\" + 0.032*\"card\" + 0.020*\"image\" + 0.020*\"driver\" + '\n",
            "  '0.020*\"problem\" + 0.019*\"run\" + 0.018*\"sale\" + 0.018*\"machine\" + '\n",
            "  '0.017*\"color\" + 0.016*\"screen\"'),\n",
            " (7,\n",
            "  '0.025*\"people\" + 0.021*\"say\" + 0.014*\"reason\" + 0.014*\"believe\" + '\n",
            "  '0.012*\"may\" + 0.012*\"evidence\" + 0.010*\"make\" + 0.010*\"think\" + '\n",
            "  '0.009*\"many\" + 0.009*\"mean\"'),\n",
            " (8,\n",
            "  '0.032*\"book\" + 0.023*\"physical\" + 0.021*\"science\" + 0.017*\"choose\" + '\n",
            "  '0.016*\"explain\" + 0.015*\"create\" + 0.011*\"author\" + 0.011*\"earth\" + '\n",
            "  '0.010*\"study\" + 0.010*\"nature\"'),\n",
            " (9,\n",
            "  '0.033*\"mail\" + 0.028*\"file\" + 0.027*\"send\" + 0.026*\"program\" + '\n",
            "  '0.025*\"thank\" + 0.024*\"information\" + 0.021*\"software\" + 0.021*\"list\" + '\n",
            "  '0.019*\"include\" + 0.019*\"address\"'),\n",
            " (10,\n",
            "  '0.073*\"group\" + 0.031*\"week\" + 0.021*\"young\" + 0.017*\"drug\" + 0.015*\"watch\" '\n",
            "  '+ 0.013*\"nntp_posting\" + 0.013*\"age\" + 0.013*\"route\" + 0.011*\"kid\" + '\n",
            "  '0.010*\"capable\"'),\n",
            " (11,\n",
            "  '0.073*\"car\" + 0.023*\"existence\" + 0.022*\"model\" + 0.020*\"engine\" + '\n",
            "  '0.016*\"pain\" + 0.012*\"keyboard\" + 0.012*\"mile\" + 0.011*\"should\" + '\n",
            "  '0.011*\"price\" + 0.011*\"insurance\"'),\n",
            " (12,\n",
            "  '0.070*\"drive\" + 0.025*\"power\" + 0.024*\"player\" + 0.017*\"speed\" + '\n",
            "  '0.017*\"light\" + 0.014*\"high\" + 0.013*\"bus\" + 0.012*\"university\" + '\n",
            "  '0.012*\"fast\" + 0.012*\"scsi\"'),\n",
            " (13,\n",
            "  '0.040*\"line\" + 0.039*\"would\" + 0.035*\"write\" + 0.024*\"article\" + 0.021*\"be\" '\n",
            "  '+ 0.020*\"get\" + 0.020*\"know\" + 0.020*\"go\" + 0.014*\"good\" + 0.014*\"think\"'),\n",
            " (14,\n",
            "  '0.027*\"patient\" + 0.017*\"family\" + 0.014*\"food\" + 0.013*\"treatment\" + '\n",
            "  '0.012*\"disease\" + 0.012*\"doctor\" + 0.011*\"cd\" + 0.011*\"diagnosis\" + '\n",
            "  '0.011*\"risk\" + 0.010*\"cause\"'),\n",
            " (15,\n",
            "  '0.029*\"wire\" + 0.021*\"ground\" + 0.021*\"eat\" + 0.019*\"material\" + '\n",
            "  '0.018*\"seller\" + 0.018*\"controller\" + 0.016*\"signal\" + 0.016*\"trust\" + '\n",
            "  '0.015*\"lead\" + 0.015*\"expensive\"'),\n",
            " (16,\n",
            "  '0.042*\"gun\" + 0.025*\"right\" + 0.016*\"carry\" + 0.015*\"law\" + 0.014*\"state\" + '\n",
            "  '0.014*\"crime\" + 0.014*\"weapon\" + 0.014*\"shoot\" + 0.013*\"steal\" + '\n",
            "  '0.013*\"protect\"'),\n",
            " (17,\n",
            "  '0.028*\"use\" + 0.025*\"system\" + 0.016*\"also\" + 0.014*\"may\" + 0.014*\"number\" '\n",
            "  '+ 0.012*\"new\" + 0.009*\"work\" + 0.009*\"support\" + 0.008*\"bit\" + '\n",
            "  '0.008*\"need\"'),\n",
            " (18,\n",
            "  '0.043*\"team\" + 0.039*\"game\" + 0.036*\"year\" + 0.030*\"play\" + 0.024*\"win\" + '\n",
            "  '0.017*\"lose\" + 0.013*\"hit\" + 0.013*\"fan\" + 0.012*\"last\" + 0.012*\"hockey\"'),\n",
            " (19,\n",
            "  '0.054*\"key\" + 0.030*\"public\" + 0.021*\"government\" + 0.017*\"internet\" + '\n",
            "  '0.015*\"encryption\" + 0.015*\"technology\" + 0.014*\"chip\" + 0.014*\"security\" + '\n",
            "  '0.014*\"instal\" + 0.013*\"private\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrzprZzfPcrD",
        "outputId": "f2b73a6c-1c97-44ec-8f82-1fcd0ad99cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Perplexity:  -8.348722848762439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0XsY-QPfXg",
        "outputId": "21e9e63f-e93a-4fbc-f825-bdccdf6ce25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.4392813747423439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OcjCSBlmpjm"
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu5oyiWA3F6-"
      },
      "source": [
        "mallet_path = '/content/mallet-2.0.8/bin/mallet'\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
        "pprint(ldamallet.show_topics(formatted=False))\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gRGqLG44M6K",
        "outputId": "36521b1a-08f3-403c-9468-09d0cec158cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.5364273910935377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97XJXFVkT1QA"
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZpaZ65gUuPc",
        "outputId": "e58eca45-50ba-43a5-a246-740db3973a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8phSFCkUllT",
        "outputId": "d68b9dbb-230d-4a4a-c60c-de2406e190ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "limit=40; start=2; step=10;\n",
        "all_topics = range(start, limit, step)\n",
        "max = 0\n",
        "optimal = 0\n",
        "for i in range(len(all_topics)):\n",
        "  if coherence_values[i]>max:\n",
        "    max = coherence_values[i]\n",
        "    optimal = all_topics[i]\n",
        "optimal"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVpNL9MGpHGl",
        "outputId": "98693725-9b4b-491c-d8b4-8b8d919f387d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=optimal, id2word=id2word)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGLKpXrkUMdB"
      },
      "source": [
        "def main_topic(sent):  \n",
        "  topic_num = {}\n",
        "  for i in model.show_topics(formatted=False, num_topics=optimal):\n",
        "    for word in sent:\n",
        "      if word in dict(i[1]).keys():\n",
        "        if i[0] in topic_num:\n",
        "          topic_num[i[0]] += dict(i[1])[word]\n",
        "        else:\n",
        "          topic_num[i[0]] = dict(i[1])[word]\n",
        "  top = Counter(topic_num).most_common(1)[0][0]\n",
        "  return top"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6zCSZglfpvj"
      },
      "source": [
        "main_topics = {}\n",
        "for text in texts:\n",
        "  try: \n",
        "    main_topics[tuple(text)] = main_topic(text)\n",
        "  except IndexError:\n",
        "    main_topics[tuple(text)] = ''"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEU2pplVoJzH",
        "outputId": "f8110651-7136-4559-f8db-49bbf572ad78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "df1 = pd.DataFrame()\n",
        "df1['text'] = list(main_topics.keys())\n",
        "df1['topic'] = list(main_topics.values())\n",
        "df1"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(where, thing, car, nntp_poste, host, park, line, wonder, could, enlighten, car, see, day, door, sport, car, look, late, early, call, bricklin, door, really, small, addition, separate, rest, body, know, tellme, model, name, engine, year, production, car, make, history, info, funky, look, car, mail, thank, bring, neighborhood, lerxst)</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(poll, final, call, summary, final, call, clock, report, keyword, acceleration, line, host, fair, number, brave, soul, upgrade, clock, oscillator, share, experience, poll, send, brief, message, detailing, experience, procedure, top, speed, attain, cpu, rate, speed, add, card, hour, usage, day, functionality, floppy, especially, request, summarize, next, day, add, network, knowledge, base, do, clock, upgrade, answer, poll, thank)</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(engineering, computer, network, distribution_usa, line, well, folk, finally, give, ghost, weekend, start, life, way, back, be, market, new, machine, bit, sooner, intend, be, look, pick, powerbook, maybe, bunch, question, hopefully, answer, know, dirt, powerbook, introduction, expect, would, hear, suppose, make, appearence, summer, hear, anymore, since, access, macleak, wonder, info, hear, rumor, price, drop, line, one, duos, go, recently, s, impression, display, could, probably, swing, get, disk, rather, really, feel, much, well, display, yea, look, great, store, really, good, could, solicit, opinion, people, day, worth, take, disk, size, money, hit, get, active, display, realize, real, subjective, question, have, play, machine, computer, ...)</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(division, line, host, write, write, article, know, chip, far, stuff, go, look, pretty, nice, get, quadrilateral, fill, command, require, point, weitek, address, phone, number, would, information, chip, division, thing, really, scare, person, sense, humor, winter)</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(question, distribution, article, write, clear, caution, warn, memory, verify, unexpected, error, wonder, expect, error, may, sorry, really, dumb, question, parity_error, memory, previously, know, condition, waivere, error, already, know, would, curious, real, meaning, expect, error, basically, know, bug, warn, system, software, thing, check, right, value, yet, set, launch, suchlike, rather, fix, code, possibly, introduce, new, bug, tell, crew, see, warn, ignore)</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11287</th>\n",
              "      <td>(scan, city, reply, line, consultation, cheap, also, well, neurologist, make, differential, diagnosis, migraine, tension, headache, cluster, benign, intracranial, hypertension, chronic, syndrome, appear, normal, scan, neurologist, also, recommend, course, treatment, appropriate, diagnosis, also, many, people, convince, brain, tumor, dn, serious, pathology, may, cheap, come, week, dn, easy, take, time, reassure, patient, personally, think, ever, justify, sigh, may, never, justifiable, sometimes, even, try, show, thoroughness, detailed, history, neurologic, examination, discussion, diagnosis, salt, lot, reassurance, patient, still, ask, can, order, scan, absolutely, sure, often, get, conversation, ignore, aunt, millie, headache, year, die, brain, tumor, aneurysm, get, away, ever, order, imaging, patient, obviously, benign, ...)</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11288</th>\n",
              "      <td>(screen, medford, old, problem, screen, blank, sometimes, minor, physical, jolt, insert, floppy, internal, drive, sometimes, computer, leave, go, blank, replace, wire, connect, logic, board, board, seem, first, jiggle, wire, make, screen, come, back, worked, blanking, return, nee, new, new, new, computer, thank, advice)</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11289</th>\n",
              "      <td>(este, mount, mail, group, line, instal, try, mount, cool, chip, hour, weight, cool, enough, dislodge, end, bend, pin, cpu, luckily, power, yet, end, press, cpu, deeply, socket, put, cpu, cooler, back, far, good, other, problem, ensure, weight, fan, heatsink, eventually, work, cpu, socket, mount, motherboard, vertical, case, este, internet)</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11290</th>\n",
              "      <td>(line, article, write, boy, embarasse, trivial, faq, give, point, find, center, point, know, circle, point, immediately, see, straightforward, way, check, geometry, book, farin, still, loss, mercy, provide, solution, would, require, space, point, specifie, sphere, far, see, prove, point, exist, space, distant, point, may, necessarily, happen, correct, be, wrong, quite, possibly, email, visit, computer, graphic, researcher)</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11291</th>\n",
              "      <td>(steal, organization, line, distribution_usa, host, summary, see, steal, plate, cbr, engine, number, turn, signal, mirror, light, tape, track, rider, spring, tomorrow, guess, help, find, baby, kjg)</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11292 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text topic\n",
              "0      (where, thing, car, nntp_poste, host, park, line, wonder, could, enlighten, car, see, day, door, sport, car, look, late, early, call, bricklin, door, really, small, addition, separate, rest, body, know, tellme, model, name, engine, year, production, car, make, history, info, funky, look, car, mail, thank, bring, neighborhood, lerxst)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        16  \n",
              "1      (poll, final, call, summary, final, call, clock, report, keyword, acceleration, line, host, fair, number, brave, soul, upgrade, clock, oscillator, share, experience, poll, send, brief, message, detailing, experience, procedure, top, speed, attain, cpu, rate, speed, add, card, hour, usage, day, functionality, floppy, especially, request, summarize, next, day, add, network, knowledge, base, do, clock, upgrade, answer, poll, thank)                                                                                                                                                                                                                                                                                                                                                                                                                       16  \n",
              "2      (engineering, computer, network, distribution_usa, line, well, folk, finally, give, ghost, weekend, start, life, way, back, be, market, new, machine, bit, sooner, intend, be, look, pick, powerbook, maybe, bunch, question, hopefully, answer, know, dirt, powerbook, introduction, expect, would, hear, suppose, make, appearence, summer, hear, anymore, since, access, macleak, wonder, info, hear, rumor, price, drop, line, one, duos, go, recently, s, impression, display, could, probably, swing, get, disk, rather, really, feel, much, well, display, yea, look, great, store, really, good, could, solicit, opinion, people, day, worth, take, disk, size, money, hit, get, active, display, realize, real, subjective, question, have, play, machine, computer, ...)                                                                                     16  \n",
              "3      (division, line, host, write, write, article, know, chip, far, stuff, go, look, pretty, nice, get, quadrilateral, fill, command, require, point, weitek, address, phone, number, would, information, chip, division, thing, really, scare, person, sense, humor, winter)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               9   \n",
              "4      (question, distribution, article, write, clear, caution, warn, memory, verify, unexpected, error, wonder, expect, error, may, sorry, really, dumb, question, parity_error, memory, previously, know, condition, waivere, error, already, know, would, curious, real, meaning, expect, error, basically, know, bug, warn, system, software, thing, check, right, value, yet, set, launch, suchlike, rather, fix, code, possibly, introduce, new, bug, tell, crew, see, warn, ignore)                                                                                                                                                                                                                                                                                                                                                                                    9   \n",
              "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...                                                                                                                                                                                                                                                                                                                                                                                   ..   \n",
              "11287  (scan, city, reply, line, consultation, cheap, also, well, neurologist, make, differential, diagnosis, migraine, tension, headache, cluster, benign, intracranial, hypertension, chronic, syndrome, appear, normal, scan, neurologist, also, recommend, course, treatment, appropriate, diagnosis, also, many, people, convince, brain, tumor, dn, serious, pathology, may, cheap, come, week, dn, easy, take, time, reassure, patient, personally, think, ever, justify, sigh, may, never, justifiable, sometimes, even, try, show, thoroughness, detailed, history, neurologic, examination, discussion, diagnosis, salt, lot, reassurance, patient, still, ask, can, order, scan, absolutely, sure, often, get, conversation, ignore, aunt, millie, headache, year, die, brain, tumor, aneurysm, get, away, ever, order, imaging, patient, obviously, benign, ...)  16  \n",
              "11288  (screen, medford, old, problem, screen, blank, sometimes, minor, physical, jolt, insert, floppy, internal, drive, sometimes, computer, leave, go, blank, replace, wire, connect, logic, board, board, seem, first, jiggle, wire, make, screen, come, back, worked, blanking, return, nee, new, new, new, computer, thank, advice)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      20  \n",
              "11289  (este, mount, mail, group, line, instal, try, mount, cool, chip, hour, weight, cool, enough, dislodge, end, bend, pin, cpu, luckily, power, yet, end, press, cpu, deeply, socket, put, cpu, cooler, back, far, good, other, problem, ensure, weight, fan, heatsink, eventually, work, cpu, socket, mount, motherboard, vertical, case, este, internet)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 16  \n",
              "11290  (line, article, write, boy, embarasse, trivial, faq, give, point, find, center, point, know, circle, point, immediately, see, straightforward, way, check, geometry, book, farin, still, loss, mercy, provide, solution, would, require, space, point, specifie, sphere, far, see, prove, point, exist, space, distant, point, may, necessarily, happen, correct, be, wrong, quite, possibly, email, visit, computer, graphic, researcher)                                                                                                                                                                                                                                                                                                                                                                                                                             9   \n",
              "11291  (steal, organization, line, distribution_usa, host, summary, see, steal, plate, cbr, engine, number, turn, signal, mirror, light, tape, track, rider, spring, tomorrow, guess, help, find, baby, kjg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  16  \n",
              "\n",
              "[11292 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdoOehB-0f5b"
      },
      "source": [
        "d = df1.groupby(['topic'])['text']"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2-jsh83A7ST"
      },
      "source": [
        "all_texts_grouped = []\n",
        "for topic in set(df1['topic']):\n",
        "  for i in d.get_group(topic).to_list():\n",
        "    sent_str = ''\n",
        "    for j in i:\n",
        "      one = []\n",
        "      sent_str += j + ' '\n",
        "      one.append(sent_str)\n",
        "    all_texts_grouped.append(one)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGzjx5vezj-T"
      },
      "source": [
        "all_main_words = []\n",
        "for g in all_grouped:\n",
        "  tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "  tfIdf = tfIdfVectorizer.fit_transform(g)\n",
        "  df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "  df = df.sort_values('TF-IDF', ascending=False)\n",
        "  all_main_words.append(list(df.index[:5]))"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJ2vOH_9nIo",
        "outputId": "cfa54675-8257-4dde-a69c-855a89d434c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_tfidf = pd.DataFrame()\n",
        "df_tfidf['main words'] = all_main_words\n",
        "df_tfidf['text'] = all_texts_grouped\n",
        "df_tfidf['topic'] = list(df1['topic'])\n",
        "df_tfidf"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>main words</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ice, afew, come, fish, know]</td>\n",
              "      <td>[night ice meaning symbolism use throw fish ice spokane afew never know come ]</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[aftersleep, look]</td>\n",
              "      <td>[aftersleep look ]</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[fool]</td>\n",
              "      <td>[fool ]</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[jumper]</td>\n",
              "      <td>[jumper ]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[failure, iisi, iron, owner, report]</td>\n",
              "      <td>[report failure iisi owner slow iron ]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11287</th>\n",
              "      <td>[chigger, thing, good, get, lot]</td>\n",
              "      <td>[jigger originator host line may world greatest expert chigger type indigenous south certainly spend lot time contemplate little bugger year move observation gain painful experience reaction chigger vary greatly person person people get tiny red bite other sensitive get fairly large swollen sore affair bite gift keep give swear thing itch month lot folklore chigger think fiction try research critter effect book could find single book uncs special collection library yet go require get base experience family member old folk remedy fingernail simply work recall_reade theory base chigger burrow skin continue party false think likely reaction toxin sort little pest release speculation good approach prevention couple thing work well good insect repellent wood liberally apply waistband good start preparation call away sulfur kind cream cortisone originally prepare army commercially_available summer put ankle morning get weekend literally can go outside live country serious consequence apparently like sulfur much sulfur dust body clothing amount prevention completely successful forget fingernail polish finally settle treatment involve topical application combination cortisone reduce inflamation swell relieve will tell thing have try tell thing wife count minor surgery best mention also think gain swell itching also significantly relieve application hot pack seem speed recovery well doctor seem care much chigger urban suburban doctor apparently encounter much rural doctor seem regard force nature must endure suspect could come good treatment chigger would make lot money principal system development ]</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11288</th>\n",
              "      <td>[thing, people, know, believe, be]</td>\n",
              "      <td>[line would share thought topic arrogance response encounter christian find dismay belief faith total truth accord belief come word truth thus know truth stance make difficult discuss faith hesitation see way truth see faith arise willful choice believe particular way choice part faith part reason seem choice discussion remind schoolyard discussion grade school kid would say policeman would ask know tell know be daddy right say s always argument usually stop right end kid grope declare belief false third time browse be cover tired old ground discussion topic pique interest welcome comment drawer be sort mystify may respond understand criticism say s enough evidence believe s good evidence religion agree clearly plenty intelligent people find evidence convincing seem point rather seem upset people believe also believe thing contradict false suggest model spiritual thing s rather different sound existentialist view people choose value follow there s actual independent spiritual reality way say specific choice unique sense right sort model modification sort may appropriate religion christianity essense historical religion base concept actual spiritual entity intervene history specific way see evidence history mundane world free choose thing work drop fall aside well define situation christian concept spiritual matter also actual external reality hope honest enough claim perfect understanding may think know confident know thing imply think thing contradict false see else could proceed need result arrogance be certainly interested talk people religion may thing teach even respect fellow get possible respect people also think matter wrong maybe even disasterously wrong clh ]</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11289</th>\n",
              "      <td>[people, gay, even, personal, society]</td>\n",
              "      <td>[many homosexual state free net line nntp_posting tired debate many gay argument basically worthless imho would really matter million people regularly deny access housing employment personal security even death threat happen know personal experience gay people far likely receive base political veiws even personal philosophy relate issue ual week go personally friend physically verbally harrasse even appear gay garaunteed certain unalienable right current form government theory yet day gay people victimize local government police force part uninforme ignorant public democracy think sense judge basis treatment people make society people include gay lesbian bisexual crime victim vary diverse society wich part arab bassoonist unite ]</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11290</th>\n",
              "      <td>[would, make, gainey, hockey, claim]</td>\n",
              "      <td>[stat organization sudbury line write gilmour take completely surprise gainey would say play technically smart hockey case claim gainey never make technical mistake absolutely ludicrous later post make reference put word people mouth would suggest last paragraph interpret way namely claim gainey never make technical mistake actually read have write find make claim soooo logic serve be contradict nonsense quite clearly state greg make claim gainey never make error make claim read hockey message would nonsense delete gainey good ever stand assessment good player belong ever watch play never make technical error nonsense delete good would display ignorance course be sure think ignorance really function lack knowledge formulating opinion need take cheap shot mean make feel better knowledgeable observer game mean obvious hockey education responsibility word vehemence poor old bother much effective player style play player laud find bothersome hate hate realize likely aesthetically pleasing player ever skate lifetime would talk gainey go around would rather check matter would take checking centre think could cover be really sorry roger lose completely ask would rather net play hockey high level good would bother bring talk hockey player can follow conversation follow say previously responsibility educate compare say example would balanced comparison sure journeyman big deal worth discuss be wrong hmmm let see wrong would take fuhr sanderson first place be guess rog feeling have setup be wrong macro key machine excellent idea decide waste time respond greg posting sure implement would suggest comment press run thing say star dynasty start hype demonstrate blanket disregard individual contributor game settle claim hockey god claim gainey eat thread know respond blanket disregard individual remember leaf team purely populate individual win team run around tell good hockey player world congenially always may consider develop style imitation sincerest form quite sure flattery intention cmon nice ring admit good laugh right get end posting realize complete joke future go respond posting would appreciate could present cogent argument support fact glean version reality rest would recognize cordially always ]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11291</th>\n",
              "      <td>[smoke, meat, carcinogenic, make, hear]</td>\n",
              "      <td>[organization tm line remind last see today smoke meat stovetop big pot use strange technique would never see take big pot lid place make aluminum foil tray size shape typical coffee table ash tray make crumpling sheet foil edge place couple spoonful brown sugar similar quantity brown rice say rice content teabags tea clove top place ordinary aluminum basket type steamer chicken breast lid put whole assembly go stovetop high heat minute later remove look smoke chicken breast surprise concern wood chip smoke flavor come years_ago remember hear color obtain caramelizing sugar common color flavor agent find carcinogenic believe inject skin rat result conclusive caramel color would legal still use initial research result find incorrect remember tea implicate carcinogenic contain oil bergamot extract skin type citrus fruit know happen story carcinogenic tea could additive yet apparently continue natural wood smoke have smoking duck right happen have notice heavily smoke food item unpleasant taste eat directly smoke recently stop flow find good taste use dry wood chip get lot smoke right begin cooking process slowly barbeque hour hour add additional wood chip theory unpleasant molecule low molecular weight stuff terpene smoky flavor molecule sort large molecule similar tar time initial intensive smoke drive low molecular weight stuff leave flavor know theory correct also remember hear combustion product fat drip charcoal burn carcinogenic reason cover product soot unpleasant tanginess meat prawn directly hot coal stuff indirect heat long weber put coal end meat end fat drop directly floor meat next time make fire end burn fat help ignite coal yet reason have hear meat smoke cure meat pork sausage bacon contain nitrosamine carcinogenic be pretty sure claim actually stand know other amuse incident recall scandal discover people make cake mix put lot ethylene dibromide know carcinogenic guy represent company defend say risk eat product day year would equal cancer risk eat charcoal broil steak great first hear immediate reaction make standard unit charcoal broil steak would equivalent duncan ]</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11292 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    main words  ... topic\n",
              "0      [ice, afew, come, fish, know]            ...  16  \n",
              "1      [aftersleep, look]                       ...  16  \n",
              "2      [fool]                                   ...  16  \n",
              "3      [jumper]                                 ...  9   \n",
              "4      [failure, iisi, iron, owner, report]     ...  9   \n",
              "...                                     ...     ... ..   \n",
              "11287  [chigger, thing, good, get, lot]         ...  16  \n",
              "11288  [thing, people, know, believe, be]       ...  20  \n",
              "11289  [people, gay, even, personal, society]   ...  16  \n",
              "11290  [would, make, gainey, hockey, claim]     ...  9   \n",
              "11291  [smoke, meat, carcinogenic, make, hear]  ...  16  \n",
              "\n",
              "[11292 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJPWr_Lk4rMy"
      },
      "source": [
        "## coherence score "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_3ogTDa4-fC"
      },
      "source": [
        "Показатели когерентности темы оценивают степень семантического сходства между словами с высокими показателями в теме. \n",
        "Для вычисления используются две метрики: внутрення UMass и внешняя UCI. Этот показатель можно сравнить с суммой ребер на полном графе.\n",
        "Внутрення метрика сравнивает как соотносится слово с соседними, а внешняя - каждое слово попарно с каждым. Для рассчета используется логарифмическая формула.\n",
        "*Источники*: \n",
        "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0  \n",
        "http://qpleple.com/topic-coherence-to-evaluate-topic-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z6RL9nxFL4K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}